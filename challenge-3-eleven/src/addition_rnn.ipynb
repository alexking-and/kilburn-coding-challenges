{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit85e9347cf0dd4c8e9244d01cbbf937f7",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Recurrent Neural Network for Performing Integer Addition</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This implementation has been adapted from an [example by Keras](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py). It is an implementation of sequence-to-sequence learning using a single LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating our training set. For a lot of ML cases, finding a tagged dataset can be a challenge; fortunately for us, integer addition is something computers are pretty alright at doing already, so we can generate a dataset as large as we like with basically no effort.\n",
    "\n",
    "We choose to reverse the input string as it's been shown to increase the model's accuracy (sources [here](https://arxiv.org/abs/1410.4615) and [here](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating training data...\nFinished generating 20000 questions\nShuffled training data\nTraining set:\nQuestions: (18000, 7, 12)\nAnswers: (18000, 4, 12)\nValidation set:\nQuestions: (2000, 7, 12)\nAnswers: (2000, 4, 12)\n"
    }
   ],
   "source": [
    "# Training data params\n",
    "TRAINING_SET_SIZE = 20000\n",
    "DIGITS_PER_SIDE = 3\n",
    "REVERSE = True\n",
    "\n",
    "MAX_INPUT_LEN = (2 * DIGITS_PER_SIDE) + 1\n",
    "CHARS = list('0123456789+ ')\n",
    "CHARS_INT_ENCODED = {x: i for i, x in enumerate(CHARS)}\n",
    "ONE_HOT_CHARS = np.array(\n",
    "    to_categorical(\n",
    "        list(CHARS_INT_ENCODED.values()), \n",
    "        len(CHARS_INT_ENCODED), \n",
    "        'bool'\n",
    "    )\n",
    ")\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "seen_questions = set()\n",
    "\n",
    "print('Generating training data...')\n",
    "while len(questions) < TRAINING_SET_SIZE:\n",
    "\n",
    "    # Generate question and answer\n",
    "    random_number_gen = lambda: int(''.join(np.random.choice(CHARS[:-2])\n",
    "        for i in range(np.random.randint(1, DIGITS_PER_SIDE + 1))))\n",
    "    a, b = random_number_gen(), random_number_gen()\n",
    "    answer_str = str(a + b)\n",
    "\n",
    "    # Skip question if duplicate\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen_questions:\n",
    "        continue\n",
    "    seen_questions.add(key)\n",
    "\n",
    "    # Build (and pad) question and answer strings\n",
    "    question_str = '{}+{}'.format(a, b)\n",
    "    question_str += ' ' * (MAX_INPUT_LEN - len(question_str))\n",
    "    answer_str += ' ' * (DIGITS_PER_SIDE + 1 - len(answer_str))\n",
    "\n",
    "    # Reverse the question string\n",
    "    if REVERSE:\n",
    "        question_str = question_str[::-1]\n",
    "\n",
    "    # Transform strings into one-hot representation\n",
    "    char_to_one_hot = lambda x: ONE_HOT_CHARS[CHARS.index(x)]\n",
    "    question_arr = np.array(list(map(char_to_one_hot, list(question_str))))\n",
    "    answer_arr = np.array(list(map(char_to_one_hot, list(answer_str))))\n",
    "\n",
    "    # Add to question/answer lists\n",
    "    questions.append(question_arr)\n",
    "    answers.append(answer_arr)\n",
    "\n",
    "# Vectorise data lists\n",
    "questions = np.array(questions)\n",
    "answers = np.array(answers)\n",
    "print('Finished generating {} questions'.format(len(questions)))\n",
    "\n",
    "# Shuffle data\n",
    "indicies = np.arange(len(questions))\n",
    "np.random.shuffle(indicies)\n",
    "questions = questions[indicies]\n",
    "answers = answers[indicies]\n",
    "print('Shuffled training data')\n",
    "\n",
    "# Partition validation set\n",
    "split_index = len(questions) - len(questions) // 10\n",
    "(questions_train, questions_val) = questions[:split_index], questions[split_index:]\n",
    "(answers_train, answers_val) = answers[:split_index], answers[split_index:]\n",
    "\n",
    "print('Training set:')\n",
    "print('Questions: {}'.format(questions_train.shape))\n",
    "print('Answers: {}'.format(answers_train.shape))\n",
    "\n",
    "print('Validation set:')\n",
    "print('Questions: {}'.format(questions_val.shape))\n",
    "print('Answers: {}'.format(answers_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some parameters for our model. Here we're using a single hidden layer of 128 nodes, and we're choosing to use LSTM for its suitability for processing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "LAYERS = 1\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our model. This works by allowing our LSTM layer to 'encode' our input into a hidden output, repeating for the length of the input, then decoding through a dense layer to retrieve our result.\n",
    "\n",
    "This network structure is chosen because some very smart data scientists did research and experiments until they got to a really accurate model. I'm not going to even pretend I fully understand it, if you want to learn more I recommend reading the references linked at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Building model...\nModel: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_12 (LSTM)               (None, 128)               72192     \n_________________________________________________________________\nrepeat_vector_6 (RepeatVecto (None, 4, 128)            0         \n_________________________________________________________________\nlstm_13 (LSTM)               (None, 4, 128)            131584    \n_________________________________________________________________\ntime_distributed_6 (TimeDist (None, 4, 12)             1548      \n=================================================================\nTotal params: 205,324\nTrainable params: 205,324\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAX_INPUT_LEN, len(CHARS))))\n",
    "model.add(layers.RepeatVector(DIGITS_PER_SIDE + 1))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(CHARS), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the data we generated earlier to train and evaluate our model, then save it so it can be used in other places for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n141/141 [==============================] - 3s 19ms/step - loss: 1.9394 - accuracy: 0.3186 - val_loss: 1.7938 - val_accuracy: 0.3470\nEpoch 2/50\n141/141 [==============================] - 2s 14ms/step - loss: 1.7643 - accuracy: 0.3599 - val_loss: 1.7531 - val_accuracy: 0.3574\nEpoch 3/50\n141/141 [==============================] - 2s 15ms/step - loss: 1.7364 - accuracy: 0.3669 - val_loss: 1.7116 - val_accuracy: 0.3739\nEpoch 4/50\n141/141 [==============================] - 2s 15ms/step - loss: 1.6769 - accuracy: 0.3842 - val_loss: 1.6717 - val_accuracy: 0.3801\nEpoch 5/50\n141/141 [==============================] - 2s 16ms/step - loss: 1.6179 - accuracy: 0.4041 - val_loss: 1.5790 - val_accuracy: 0.4191\nEpoch 6/50\n141/141 [==============================] - 2s 17ms/step - loss: 1.5568 - accuracy: 0.4259 - val_loss: 1.5133 - val_accuracy: 0.4486\nEpoch 7/50\n141/141 [==============================] - 3s 18ms/step - loss: 1.4662 - accuracy: 0.4610 - val_loss: 1.4082 - val_accuracy: 0.4776\nEpoch 8/50\n141/141 [==============================] - 2s 17ms/step - loss: 1.3661 - accuracy: 0.4909 - val_loss: 1.3244 - val_accuracy: 0.5015\nEpoch 9/50\n141/141 [==============================] - 2s 17ms/step - loss: 1.2847 - accuracy: 0.5163 - val_loss: 1.2621 - val_accuracy: 0.5247\nEpoch 10/50\n141/141 [==============================] - 2s 17ms/step - loss: 1.2206 - accuracy: 0.5416 - val_loss: 1.1855 - val_accuracy: 0.5554\nEpoch 11/50\n141/141 [==============================] - 2s 17ms/step - loss: 1.1637 - accuracy: 0.5643 - val_loss: 1.1281 - val_accuracy: 0.5814\nEpoch 12/50\n141/141 [==============================] - 3s 18ms/step - loss: 1.1100 - accuracy: 0.5870 - val_loss: 1.0875 - val_accuracy: 0.5947\nEpoch 13/50\n141/141 [==============================] - 2s 16ms/step - loss: 1.0630 - accuracy: 0.6072 - val_loss: 1.0497 - val_accuracy: 0.6096\nEpoch 14/50\n141/141 [==============================] - 2s 16ms/step - loss: 1.0256 - accuracy: 0.6228 - val_loss: 1.0147 - val_accuracy: 0.6186\nEpoch 15/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.9855 - accuracy: 0.6391 - val_loss: 0.9838 - val_accuracy: 0.6341\nEpoch 16/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.9561 - accuracy: 0.6509 - val_loss: 0.9480 - val_accuracy: 0.6500\nEpoch 17/50\n141/141 [==============================] - 2s 15ms/step - loss: 0.9159 - accuracy: 0.6675 - val_loss: 0.9170 - val_accuracy: 0.6644\nEpoch 18/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.8873 - accuracy: 0.6795 - val_loss: 0.8878 - val_accuracy: 0.6764\nEpoch 19/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.8571 - accuracy: 0.6923 - val_loss: 0.8725 - val_accuracy: 0.6710\nEpoch 20/50\n141/141 [==============================] - 2s 18ms/step - loss: 0.8294 - accuracy: 0.7048 - val_loss: 0.8329 - val_accuracy: 0.6969\nEpoch 21/50\n141/141 [==============================] - 3s 18ms/step - loss: 0.7963 - accuracy: 0.7184 - val_loss: 0.8004 - val_accuracy: 0.7067\nEpoch 22/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.7703 - accuracy: 0.7277 - val_loss: 0.7809 - val_accuracy: 0.7153\nEpoch 23/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.7432 - accuracy: 0.7390 - val_loss: 0.7503 - val_accuracy: 0.7271\nEpoch 24/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.7149 - accuracy: 0.7511 - val_loss: 0.7204 - val_accuracy: 0.7390\nEpoch 25/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.6936 - accuracy: 0.7573 - val_loss: 0.7010 - val_accuracy: 0.7477\nEpoch 26/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.6588 - accuracy: 0.7741 - val_loss: 0.6717 - val_accuracy: 0.7609\nEpoch 27/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.6370 - accuracy: 0.7809 - val_loss: 0.6568 - val_accuracy: 0.7629\nEpoch 28/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.6130 - accuracy: 0.7882 - val_loss: 0.6320 - val_accuracy: 0.7686\nEpoch 29/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.5819 - accuracy: 0.8011 - val_loss: 0.5941 - val_accuracy: 0.7894\nEpoch 30/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.5483 - accuracy: 0.8121 - val_loss: 0.5734 - val_accuracy: 0.7895\nEpoch 31/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.5213 - accuracy: 0.8189 - val_loss: 0.5310 - val_accuracy: 0.8081\nEpoch 32/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.4856 - accuracy: 0.8324 - val_loss: 0.5071 - val_accuracy: 0.8120\nEpoch 33/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.4489 - accuracy: 0.8474 - val_loss: 0.4605 - val_accuracy: 0.8359\nEpoch 34/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.4132 - accuracy: 0.8584 - val_loss: 0.4336 - val_accuracy: 0.8406\nEpoch 35/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.3776 - accuracy: 0.8730 - val_loss: 0.4458 - val_accuracy: 0.8261\nEpoch 36/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.3427 - accuracy: 0.8881 - val_loss: 0.3661 - val_accuracy: 0.8669\nEpoch 37/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.3097 - accuracy: 0.9034 - val_loss: 0.3299 - val_accuracy: 0.8859\nEpoch 38/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.2859 - accuracy: 0.9122 - val_loss: 0.2999 - val_accuracy: 0.9001\nEpoch 39/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.2497 - accuracy: 0.9307 - val_loss: 0.2827 - val_accuracy: 0.9059\nEpoch 40/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.2269 - accuracy: 0.9386 - val_loss: 0.2510 - val_accuracy: 0.9205\nEpoch 41/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.2093 - accuracy: 0.9454 - val_loss: 0.2275 - val_accuracy: 0.9336\nEpoch 42/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.1858 - accuracy: 0.9545 - val_loss: 0.2067 - val_accuracy: 0.9389\nEpoch 43/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.1603 - accuracy: 0.9657 - val_loss: 0.1945 - val_accuracy: 0.9434\nEpoch 44/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.1466 - accuracy: 0.9697 - val_loss: 0.1767 - val_accuracy: 0.9490\nEpoch 45/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.1400 - accuracy: 0.9693 - val_loss: 0.1684 - val_accuracy: 0.9556\nEpoch 46/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.1256 - accuracy: 0.9750 - val_loss: 0.1625 - val_accuracy: 0.9507\nEpoch 47/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.1118 - accuracy: 0.9794 - val_loss: 0.1473 - val_accuracy: 0.9576\nEpoch 48/50\n141/141 [==============================] - 2s 17ms/step - loss: 0.1055 - accuracy: 0.9801 - val_loss: 0.1292 - val_accuracy: 0.9671\nEpoch 49/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.0884 - accuracy: 0.9866 - val_loss: 0.1211 - val_accuracy: 0.9679\nEpoch 50/50\n141/141 [==============================] - 2s 16ms/step - loss: 0.0906 - accuracy: 0.9828 - val_loss: 0.1441 - val_accuracy: 0.9576\nTraining completed\nSaving model to  ./data/model  ...\nINFO:tensorflow:Assets written to: ./data/model/assets\nModel saved\n"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "SAVE_PATH = './data/model'\n",
    "\n",
    "model.fit(\n",
    "    questions_train, \n",
    "    answers_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(questions_val, answers_val)\n",
    ")\n",
    "print('Training completed')\n",
    "\n",
    "print('Saving model to ', SAVE_PATH, ' ...')\n",
    "model.save(SAVE_PATH)\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Some more tinkering with our variables would be needed to get us a model as close to perfect as we could get, but with 20k training examples and 50 epochs, we're seeing an accuracies around 97%. With our model, we can accurately add together almost ANY pair of integers less than 1000. **Your move, Google**.\n",
    "\n",
    "Below we use the model to obtain the answer to our question \"what is 1 + 1?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The predicted answer is  2   \n"
    }
   ],
   "source": [
    "# Encode '1+1' into input format and transform with model\n",
    "char_to_one_hot = lambda x: ONE_HOT_CHARS[CHARS.index(x)]\n",
    "input_q = list(map(char_to_one_hot, list('    1+1')))\n",
    "input_q = tf.convert_to_tensor([input_q])\n",
    "output = model(input_q)\n",
    "\n",
    "# Convert result into a string\n",
    "prediction = tf.argmax(output[0], axis=1, output_type=tf.int32)\n",
    "prediction_string = ''.join(list(map(lambda x: CHARS[x], list(prediction.numpy()))))\n",
    "print('The predicted answer is ', prediction_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Addition RNN example by Keras](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py) \n",
    "\n",
    "2. [Sequence to Sequence Learning with Neural Networks - Sutskever, Vinyals, Le](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "\n",
    "3. [Learning to Execute - Zaremba, Sutskever](https://arxiv.org/abs/1410.4615)"
   ]
  }
 ]
}